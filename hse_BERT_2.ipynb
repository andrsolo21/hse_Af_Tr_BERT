{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hse_BERT_2.ipynb","provenance":[],"authorship_tag":"ABX9TyPwLJ7RPMUAERix8PVgcbtK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wFOv_GdndRBf"},"source":["# Подготовка среды"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNIRbsjvXTT1","executionInfo":{"status":"ok","timestamp":1612866959588,"user_tz":-180,"elapsed":23274,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"27b342d8-c446-41af-8262-e6d9e3c75036"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e3Jn3fsZ3oVD","executionInfo":{"status":"ok","timestamp":1612866959590,"user_tz":-180,"elapsed":23251,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import pickle as pc\r\n","import numpy as np\r\n","import numpy"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrMRUKMUZC-W","executionInfo":{"status":"ok","timestamp":1612866959950,"user_tz":-180,"elapsed":23589,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import os\r\n","os.chdir('/content/drive/Shared drives/hse_BERT/hse_Af_Tr_BERT')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXjr0XyrdpLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612866967887,"user_tz":-180,"elapsed":31505,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"14a00b86-84f3-44b5-bcb0-3b6376d7d4e0"},"source":["!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/89/f07e7a884072ad37b1b6b1578637ab36152e0251d74abb950d967a59904e/transformers-4.3.1-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 12.5MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 42.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=7ab7ff2cb42bf69ee17e492c509d41be03fea959911a879503fc2f579794221b\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrzflXz2wdQG","executionInfo":{"status":"ok","timestamp":1612866970746,"user_tz":-180,"elapsed":34345,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"4c8c83de-e450-4f2e-bd45-487e989befdf"},"source":["!pip install razdel"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting razdel\n","  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n","Installing collected packages: razdel\n","Successfully installed razdel-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zSA0MOOMTPgk","executionInfo":{"status":"ok","timestamp":1612866975496,"user_tz":-180,"elapsed":39078,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import torch\r\n","import nltk\r\n","import re\r\n","import json\r\n","import time\r\n","import numpy as np\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ge0GhZA0zzE"},"source":["# read embeddings"]},{"cell_type":"code","metadata":{"id":"OJxSs8oa00QK","executionInfo":{"status":"ok","timestamp":1612866987384,"user_tz":-180,"elapsed":2733,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["files = os.listdir(\"./embeddings/sent1/bin\")\r\n","cut_files = []\r\n","for i in files:\r\n","    if i.split('.')[-1] == 'pic':\r\n","        cut_files.append(i)\r\n","# cut_files"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_kbzYi51sTW","executionInfo":{"status":"ok","timestamp":1612866987388,"user_tz":-180,"elapsed":2439,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["# i = \"1000_00024.pic\"\r\n","# for i in cut_files:\r\n","#     with open(\"./embeddings/sent1/bin/{}\".format(i), 'rb') as f:\r\n","#         data = pc.load(f)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHu7WxpNH0Fe"},"source":["##get count"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pV-8Gucl1sZd","executionInfo":{"status":"ok","timestamp":1612867906576,"user_tz":-180,"elapsed":921108,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"7b4999d0-e33c-498a-c137-8cc37146655f"},"source":["%%time\r\n","from_files = {}\r\n","all_count = {}\r\n","\r\n","for name in cut_files:\r\n","    with open(\"./embeddings/sent1/bin/{}\".format(name), 'rb') as f:\r\n","        data = pc.load(f)\r\n","    for i in data.keys():\r\n","        if i not in all_count:\r\n","            all_count[i] = len(data[i])\r\n","            from_files[i] = [name]\r\n","        else:\r\n","            all_count[i] += len(data[i])\r\n","            from_files[i].append(name)\r\n","    # print(name)\r\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CPU times: user 10min 11s, sys: 31.5 s, total: 10min 42s\n","Wall time: 15min 19s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QS6qXPsK1sc_","executionInfo":{"status":"ok","timestamp":1612867906578,"user_tz":-180,"elapsed":920928,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"761aa7e2-99e2-4c92-ed1a-fdf38efdb818"},"source":["out = list(zip(all_count.keys(), list(all_count.values())))\r\n","out.sort(key = lambda i: i[1], reverse=True)\r\n","out[:20]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('как', 87089),\n"," ('что', 87073),\n"," ('его', 62700),\n"," ('the', 54443),\n"," ('в', 49243),\n"," ('и', 44503),\n"," ('это', 43184),\n"," ('для', 39351),\n"," ('так', 27947),\n"," ('все', 25799),\n"," ('или', 23508),\n"," ('с', 23001),\n"," ('только', 22587),\n"," ('and', 21309),\n"," ('жизни', 21113),\n"," ('она', 17073),\n"," ('том', 14964),\n"," ('этом', 14789),\n"," ('они', 14699),\n"," ('при', 14630)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvOtbW-6a4np","executionInfo":{"status":"ok","timestamp":1612867906580,"user_tz":-180,"elapsed":920320,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"5621d829-9d0b-41b0-80ef-2b1a78a45684"},"source":["keys = list(all_count.keys())\r\n","k1 = []\r\n","k2 = []\r\n","k3 = []\r\n","kb = []\r\n","l = 0\r\n","for i in keys:\r\n","    l = len(i)\r\n","    if l == 1:\r\n","        k1.append(i)\r\n","    elif l == 2:\r\n","        k2.append(i)\r\n","    elif l == 3:\r\n","        k3.append(i)\r\n","    else:\r\n","        kb.append(i)\r\n","len(k1),len(k2),len(k3),len(kb)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(80, 549, 2092, 52515)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIHeXRv2cbM6","executionInfo":{"status":"ok","timestamp":1612867906581,"user_tz":-180,"elapsed":920238,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"8eb14ac4-14c9-4a08-9c85-4312e09af2e4"},"source":["all_words = []\r\n","for i in kb:\r\n","    if all_count[i] > 10:\r\n","        all_words.append(i)\r\n","len(all_words)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37335"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"zzUkB5fhHbDt","executionInfo":{"status":"ok","timestamp":1612867906583,"user_tz":-180,"elapsed":919827,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["def check_existing_emb(words):\r\n","    d = dict(out)\r\n","    fl = True\r\n","    for i in words:\r\n","        if i not in d:\r\n","            print(\"{} not in dictionary\".format(i))\r\n","            fl = False\r\n","    return fl\r\n","\r\n","def get_all_embeddingd(words):\r\n","    all_files = []\r\n","    embeddings = {}\r\n","    for w in words:\r\n","        embeddings[w] = []\r\n","\r\n","    for i in words:\r\n","        all_files += from_files[i]\r\n","\r\n","    for name in set(all_files):\r\n","        with open(\"./embeddings/sent1/bin/{}\".format(name), 'rb') as f:\r\n","            data = pc.load(f)\r\n","        for w in words:\r\n","            if w in data:\r\n","                embeddings[w] += data[w]\r\n","    return embeddings\r\n","\r\n","def sum_embeddings(embeddings):\r\n","    res = {}\r\n","    res_emb = {}\r\n","    for i in embeddings:\r\n","        res[i] = torch.empty(len(embeddings[i]), embeddings[i][0].size(0))\r\n","        for j in range(len(embeddings[i])):\r\n","            res[i][j] = embeddings[i][j]\r\n","        res_emb[i] = res[i].mean(dim = 0)\r\n","    return res_emb"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgRmaNBcBu_J","outputId":"c579ee33-93f6-4f4a-bf74-da88c2775a4f"},"source":["%%time\r\n","st_time = time.time()\r\n","all_emb = {}\r\n","batch = 100\r\n","for b in range(143, len(all_words)//batch):\r\n","    t_time = time.time()\r\n","    words = all_words[b*batch:(b+1)*batch]\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)\r\n","    \r\n","    with open(\"./embeddings/sent1/ready/{}_{:05d}.pic\".format(batch, b), 'wb') as f:\r\n","        pc.dump(res_emb, f)\r\n","\r\n","    print(\"done: {} --- time: {}\\t--- total: {}\".format(b,time.time() - t_time, time.time() - st_time))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["done: 143 --- time: 861.7812881469727\t--- total: 861.7813138961792\n","done: 144 --- time: 873.3714110851288\t--- total: 1735.1582100391388\n","done: 145 --- time: 885.7795269489288\t--- total: 2620.9453070163727\n","done: 146 --- time: 884.5825431346893\t--- total: 3505.5360839366913\n","done: 147 --- time: 896.6011607646942\t--- total: 4402.146082401276\n","done: 148 --- time: 905.8998055458069\t--- total: 5308.057395458221\n","done: 149 --- time: 911.9224762916565\t--- total: 6219.985296010971\n","done: 150 --- time: 912.3649122714996\t--- total: 7132.357630252838\n","done: 151 --- time: 911.4052352905273\t--- total: 8043.767138719559\n","done: 152 --- time: 896.9978258609772\t--- total: 8940.76932644844\n","done: 153 --- time: 894.015456199646\t--- total: 9834.79453086853\n","done: 154 --- time: 908.3928968906403\t--- total: 10743.192079305649\n","done: 155 --- time: 899.9152212142944\t--- total: 11643.111984491348\n","done: 156 --- time: 907.0629441738129\t--- total: 12550.183970928192\n","done: 157 --- time: 903.3286819458008\t--- total: 13453.517285346985\n","done: 158 --- time: 919.1203486919403\t--- total: 14372.646310091019\n","done: 159 --- time: 902.3438086509705\t--- total: 15275.000244617462\n","done: 160 --- time: 919.1512022018433\t--- total: 16194.162677288055\n","done: 161 --- time: 920.2216467857361\t--- total: 17114.3923330307\n","done: 162 --- time: 906.4021949768066\t--- total: 18020.80357527733\n","done: 163 --- time: 888.8584291934967\t--- total: 18909.6708214283\n","done: 164 --- time: 912.14808344841\t--- total: 19821.82350373268\n","done: 165 --- time: 894.8707845211029\t--- total: 20716.69917035103\n","done: 166 --- time: 905.1615223884583\t--- total: 21621.871457338333\n","done: 167 --- time: 897.5057363510132\t--- total: 22519.386555433273\n","done: 168 --- time: 913.1831312179565\t--- total: 23432.579080343246\n","done: 169 --- time: 901.7715735435486\t--- total: 24334.35658812523\n","done: 170 --- time: 904.1367530822754\t--- total: 25238.505081176758\n","done: 171 --- time: 892.6192963123322\t--- total: 26131.12964105606\n","done: 172 --- time: 924.6684885025024\t--- total: 27055.802347421646\n","done: 173 --- time: 899.1109454631805\t--- total: 27954.921468257904\n","done: 174 --- time: 915.4246008396149\t--- total: 28870.355397462845\n","done: 175 --- time: 901.4152467250824\t--- total: 29771.787387132645\n","done: 176 --- time: 919.4747939109802\t--- total: 30691.26831793785\n","done: 177 --- time: 903.7304880619049\t--- total: 31595.011026382446\n","done: 178 --- time: 885.7984457015991\t--- total: 32480.814675569534\n","done: 179 --- time: 911.3692300319672\t--- total: 33392.18827533722\n","done: 180 --- time: 901.3158991336823\t--- total: 34293.51200890541\n","done: 181 --- time: 895.6072423458099\t--- total: 35189.12437438965\n","done: 182 --- time: 881.4543330669403\t--- total: 36070.58289504051\n","done: 183 --- time: 901.9523248672485\t--- total: 36972.5444791317\n","done: 184 --- time: 886.4189450740814\t--- total: 37858.96927666664\n","done: 185 --- time: 900.3140707015991\t--- total: 38759.31274342537\n","done: 186 --- time: 890.690835237503\t--- total: 39650.08414769173\n","done: 187 --- time: 894.0749537944794\t--- total: 40544.17058253288\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qgbWPFkV8Ctt"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3F7kbbmrH4sT"},"source":["## get embeddings"]},{"cell_type":"code","metadata":{"id":"z5KEphwROsy_"},"source":["%%time\r\n","# words = ['мать', 'сын', 'отец', 'дочь', 'парень', 'девушка', 'машина', 'автомобиль', 'самолет', 'ребенок', 'королева', 'король', 'лед', 'тепло', 'вода']\r\n","words = ['россия','москв','франци','париж','украин','киев']\r\n","\r\n","if check_existing_emb(words):\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xwUIz_vUNsm"},"source":["t1 = res_emb['франци'] - res_emb['париж'] + res_emb['киев']\r\n","t2 = res_emb['украин']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuB9zah9UN9F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2ipTLE-UOCG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMcKl9gPRkjY"},"source":["mother = res_emb['мать']\r\n","father = res_emb['отец']\r\n","sun = res_emb['сын']\r\n","dot = res_emb['дочь']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTA0HYGB-h_Y"},"source":["t1 = res_emb['девушка'] - res_emb['парень'] + res_emb['сын']\r\n","t2 = res_emb['дочь']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uElQlw28IbbD"},"source":["t1 = res_emb['король'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['королева']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Y2OMQFcIvtb"},"source":["t1 = res_emb['король'] - res_emb['королева']\r\n","t2 = res_emb['девушка'] - res_emb['парень']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kH7PgjWtB9eY"},"source":["t1 = res_emb['отец'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['мать']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SSwhPJm-nqL"},"source":["t1 = res_emb['девушка']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PrvWbb7B3HC"},"source":["t1 = res_emb['машина']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnurMtwbUy78"},"source":["t1 = mother - father + sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzR_wCkPUzAn"},"source":["t1 = mother\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ot_kYH5cUzDh"},"source":["t1 = father\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DJuzMl3UzGy"},"source":["t1 = sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trqPoVJbUzJo"},"source":["t1 = mother\r\n","t2 = father\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lZ3FMjEcUP4"},"source":["t1 = mother\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_mmvAJ1cUT7"},"source":["t1 = father\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTzf59SKXjex"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJ1Jbokqml8X"},"source":["# short "]},{"cell_type":"markdown","metadata":{"id":"mSnTVO95wEqL"},"source":["## imports"]},{"cell_type":"code","metadata":{"id":"iCBwhjwymmux"},"source":["import torch\r\n","import nltk\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiaAhMe7nU6N"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0SKv0m8warN"},"source":["from razdel import sentenize\r\n","# list(sentenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n98R8rEWxSOJ"},"source":["\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","# sentence = \"\"\"At eight o'clock on Thursday morning. Arthur didn't feel very good.\"\"\"\r\n","tokens = nltk.word_tokenize(sentences[1])\r\n","tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lj1Lu9WPyRk1"},"source":["sentences = nltk.sent_tokenize(text)\r\n","sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqaOLbQ3xb1V"},"source":["tagged = nltk.pos_tag(tokens)\r\n","tagged[0:6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADtaVlDjwH9p"},"source":["## work"]},{"cell_type":"code","metadata":{"id":"A_5phDHumt1i"},"source":["text = 'Замок стоял неприступный. Замок легко открылся.'\r\n","text2 = \"At eight o'clock on Thursday morning. Arthur didn't feel very good.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2BCGpfIeO1G"},"source":["# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n","\r\n","# marked_text2 = \"[CLS] \" + \"Работает по следующему принципу\" + \" [SEP]\"\r\n","# tokenized_text2 = tokenizer.tokenize(marked_text2)\r\n","# tokenized_text2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1TawqaTmt5A"},"source":["marked_text2 = text.lower()\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","# tokenized_text = nltk.word_tokenize(marked_text)\r\n","\r\n","# tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\r\n","\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKlj14WFaL6V"},"source":["tokenized_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wPr8IKKmt_3"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f0yAQrsnigF"},"source":["with torch.no_grad():\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQOs50kOnisy"},"source":["# token_i = 5\r\n","# layer_i = 5\r\n","# vec = hidden_states[layer_i][batch_i][token_i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfKPm4a2gMRc"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFmJdLgJnui8"},"source":["# Remove dimension 1, the \"batches\".\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1).permute(1,0,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh65R4Mun7dX"},"source":["token_vecs_sum = []\r\n","for token in token_embeddings:\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5b6OwZDqqhR"},"source":["len(token_vecs_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za7aF1iQrYJE"},"source":["for i, token_str in enumerate(tokenized_text):\r\n","  print (i, token_str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThBR9wTfoA4k"},"source":["token_vecs = hidden_states[-2][0]\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEjdLKPEbm2J"},"source":["token_vecs.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfHWSz36oA78"},"source":["diff_bank = 1 - cosine(token_vecs_sum[1], token_vecs_sum[7])\r\n","diff_bank1 = 1 - cosine(token_vecs[1], token_vecs[7])\r\n","# same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank1)\r\n","# print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXtCWy7idfs5"},"source":["# Baseline\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"1vz_xfmsdqX_"},"source":["import torch\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CaU-dkwpdyar"},"source":["text = \"Here is the sentence I want embeddings for.\"\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Tokenize our sentence with the BERT tokenizer.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Print out the tokens.\r\n","print (tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDdQ7uDRd5R4"},"source":["# Define a new example sentence with multiple meanings of the word \"bank\"\r\n","text = \"After stealing money from the bank vault, the bank robber was seen \" \\\r\n","       \"fishing on the Mississippi river bank.\"\r\n","\r\n","# Add the special tokens.\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Split the sentence into tokens.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Map the token strings to their vocabulary indeces.\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","\r\n","# Display the words with their indeces.\r\n","for tup in zip(tokenized_text, indexed_tokens):\r\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sQM8BeAeP0Z"},"source":["# Mark each of the 22 tokens as belonging to sentence \"1\".\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcZ2TlS6eCHo"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fWcVbNNeMHa"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('bert-base-uncased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp_jYk_5eUB3"},"source":["# Run the text through BERT, and collect all of the hidden states produced\r\n","# from all 12 layers. \r\n","with torch.no_grad():\r\n","\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","\r\n","    # Evaluating the model will return a different number of objects based on \r\n","    # how it's  configured in the `from_pretrained` call earlier. In this case, \r\n","    # becase we set `output_hidden_states = True`, the third item will be the \r\n","    # hidden states from all layers. See the documentation for more details:\r\n","    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOImPmmufFeO"},"source":["# outputs.shape\r\n","hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlk64upJfIP4"},"source":["print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\r\n","layer_i = 0\r\n","\r\n","print (\"Number of batches:\", len(hidden_states[layer_i]))\r\n","batch_i = 0\r\n","\r\n","print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\r\n","token_i = 0\r\n","\r\n","print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYDKqBYNhOGZ"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD82_lhOky7g"},"source":["# Remove dimension 1, the \"batches\".\r\n","\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\r\n","\r\n","token_embeddings.size()\r\n","\r\n","# Swap dimensions 0 and 1.\r\n","token_embeddings = token_embeddings.permute(1,0,2)\r\n","\r\n","token_embeddings.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UT-Pbq7hhzV"},"source":["# Stores the token vectors, with shape [22 x 3,072]\r\n","token_vecs_cat = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","    \r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Concatenate the vectors (that is, append them together) from the last \r\n","    # four layers.\r\n","    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\r\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\r\n","    \r\n","    # Use `cat_vec` to represent `token`.\r\n","    token_vecs_cat.append(cat_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2mSMHsghUkx"},"source":["# Stores the token vectors, with shape [22 x 768]\r\n","token_vecs_sum = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","\r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Sum the vectors from the last four layers.\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    \r\n","    # Use `sum_vec` to represent `token`.\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2ZHmxSFlNDt"},"source":["# `hidden_states` has shape [13 x 1 x 22 x 768]\r\n","\r\n","# `token_vecs` is a tensor with shape [22 x 768]\r\n","token_vecs = hidden_states[-2][0]\r\n","\r\n","# Calculate the average of all 22 token vectors.\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRdzQAmilRws"},"source":["from scipy.spatial.distance import cosine\r\n","\r\n","# Calculate the cosine similarity between the word bank \r\n","# in \"bank robber\" vs \"river bank\" (different meanings).\r\n","diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\r\n","\r\n","# Calculate the cosine similarity between the word bank\r\n","# in \"bank robber\" vs \"bank vault\" (same meaning).\r\n","same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\r\n","print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sLrZsOclZSI"},"source":[""],"execution_count":null,"outputs":[]}]}