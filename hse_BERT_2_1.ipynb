{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hse_BERT_2_1.ipynb","provenance":[{"file_id":"1AtbtIhxWMBlu2gfk3SIMM1nMn5Ce4A8q","timestamp":1612738299260}],"toc_visible":true,"authorship_tag":"ABX9TyPEweiM9Z3O8BFLtRJEWIc3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wFOv_GdndRBf"},"source":["# Подготовка среды"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNIRbsjvXTT1","executionInfo":{"status":"ok","timestamp":1612988581184,"user_tz":-180,"elapsed":25524,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"d5d33d06-503b-42fa-a313-e0ff374ea2c4"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e3Jn3fsZ3oVD"},"source":["import pickle as pc\r\n","import numpy as np\r\n","import numpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrMRUKMUZC-W"},"source":["import os\r\n","os.chdir('/content/drive/Shared drives/hse_BERT/hse_Af_Tr_BERT')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXjr0XyrdpLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612988590412,"user_tz":-180,"elapsed":34709,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"557e360f-0330-4fd5-9ddf-bb8e92bbab06"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n","\r\u001b[K     |▏                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 23.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 16.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 13.5MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 15.8MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 13.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 12.0MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 143kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153kB 12.0MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 12.0MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 194kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215kB 12.0MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 12.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235kB 12.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245kB 12.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 256kB 12.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266kB 12.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 276kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 286kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 307kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 317kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 327kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 337kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 348kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 358kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 368kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 378kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 389kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 399kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 409kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 419kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 430kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 440kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 450kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 460kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 471kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 481kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 491kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 501kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 512kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 522kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 532kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 542kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 552kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 573kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 583kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 593kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 604kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 614kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 624kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 634kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 645kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 655kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 665kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 686kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 696kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 706kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 716kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 727kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 737kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 747kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 757kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 768kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 778kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 788kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 798kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 808kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 819kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 829kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 839kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 849kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 860kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 870kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 880kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 890kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 901kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 911kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 921kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 931kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 942kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 952kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 962kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 972kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 983kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 993kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.0MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.7MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.8MB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 12.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 57.3MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 67.3MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2fe072988e3c96286905960f031749d30f0df70af1b0ae041e41e12d382574f6\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrzflXz2wdQG","executionInfo":{"status":"ok","timestamp":1612988593112,"user_tz":-180,"elapsed":37397,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"2a952cb4-13be-4a6c-9e10-c70acf9223ef"},"source":["!pip install razdel"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting razdel\n","  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n","Installing collected packages: razdel\n","Successfully installed razdel-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zSA0MOOMTPgk"},"source":["import torch\r\n","import nltk\r\n","import re\r\n","import json\r\n","import time\r\n","import numpy as np\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3ZzpNgDnHy_"},"source":["# one dict"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-plr0JpnRbw","executionInfo":{"status":"ok","timestamp":1612989232688,"user_tz":-180,"elapsed":213051,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"baa1a8e9-d56e-40da-b69e-5dadaf74481a"},"source":["files = os.listdir(\"./embeddings/sent1/ready/\")\r\n","all_emb = {}\r\n","for name in files:\r\n","    with open(\"./embeddings/sent1/ready/{}\".format(name), 'rb') as f:\r\n","        data = pc.load(f)\r\n","        all_emb = {**all_emb, **data}\r\n","len(all_emb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37300"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"X_xCP_sVoJ5Y"},"source":["with open(\"./embeddings/sent1/all_emc.pic\", 'wb') as f:\r\n","    pc.dump(all_emb, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"di4sS_NkwLun"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ge0GhZA0zzE"},"source":["# read embeddings"]},{"cell_type":"code","metadata":{"id":"OJxSs8oa00QK"},"source":["files = os.listdir(\"./embeddings/sent1/bin\")\r\n","cut_files = []\r\n","for i in files:\r\n","    if i.split('.')[-1] == 'pic':\r\n","        cut_files.append(i)\r\n","# cut_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_kbzYi51sTW"},"source":["# i = \"1000_00024.pic\"\r\n","# for i in cut_files:\r\n","#     with open(\"./embeddings/sent1/bin/{}\".format(i), 'rb') as f:\r\n","#         data = pc.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHu7WxpNH0Fe"},"source":["##get count"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pV-8Gucl1sZd","executionInfo":{"status":"ok","timestamp":1612867881852,"user_tz":-180,"elapsed":907202,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"45c9f8b0-7a42-439e-ef53-eab24968b670"},"source":["%%time\r\n","from_files = {}\r\n","all_count = {}\r\n","\r\n","for name in cut_files:\r\n","    with open(\"./embeddings/sent1/bin/{}\".format(name), 'rb') as f:\r\n","        data = pc.load(f)\r\n","    for i in data.keys():\r\n","        if i not in all_count:\r\n","            all_count[i] = len(data[i])\r\n","            from_files[i] = [name]\r\n","        else:\r\n","            all_count[i] += len(data[i])\r\n","            from_files[i].append(name)\r\n","    # print(name)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 10min 25s, sys: 33.2 s, total: 10min 58s\n","Wall time: 14min 47s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QS6qXPsK1sc_","executionInfo":{"status":"ok","timestamp":1612867881855,"user_tz":-180,"elapsed":907033,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"c3d1cce9-e549-4341-9ef5-c4f2d7108fb1"},"source":["out = list(zip(all_count.keys(), list(all_count.values())))\r\n","out.sort(key = lambda i: i[1], reverse=True)\r\n","out[:20]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('как', 87089),\n"," ('что', 87073),\n"," ('его', 62700),\n"," ('the', 54443),\n"," ('в', 49243),\n"," ('и', 44503),\n"," ('это', 43184),\n"," ('для', 39351),\n"," ('так', 27947),\n"," ('все', 25799),\n"," ('или', 23508),\n"," ('с', 23001),\n"," ('только', 22587),\n"," ('and', 21309),\n"," ('жизни', 21113),\n"," ('она', 17073),\n"," ('том', 14964),\n"," ('этом', 14789),\n"," ('они', 14699),\n"," ('при', 14630)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvOtbW-6a4np","executionInfo":{"status":"ok","timestamp":1612867881857,"user_tz":-180,"elapsed":906929,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"73e3708f-5418-422f-b7b8-1014d26fc19b"},"source":["keys = list(all_count.keys())\r\n","k1 = []\r\n","k2 = []\r\n","k3 = []\r\n","kb = []\r\n","l = 0\r\n","for i in keys:\r\n","    l = len(i)\r\n","    if l == 1:\r\n","        k1.append(i)\r\n","    elif l == 2:\r\n","        k2.append(i)\r\n","    elif l == 3:\r\n","        k3.append(i)\r\n","    else:\r\n","        kb.append(i)\r\n","len(k1),len(k2),len(k3),len(kb)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(80, 549, 2092, 52515)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIHeXRv2cbM6","executionInfo":{"status":"ok","timestamp":1612867881859,"user_tz":-180,"elapsed":906075,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"c509a29d-4ef4-46e1-9f3d-3d88a1929732"},"source":["all_words = []\r\n","for i in kb:\r\n","    if all_count[i] > 10:\r\n","        all_words.append(i)\r\n","len(all_words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37335"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"mdKYr9Bps891"},"source":["qwe = list(range(225))[::-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzUkB5fhHbDt"},"source":["def check_existing_emb(words):\r\n","    d = dict(out)\r\n","    fl = True\r\n","    for i in words:\r\n","        if i not in d:\r\n","            print(\"{} not in dictionary\".format(i))\r\n","            fl = False\r\n","    return fl\r\n","\r\n","def get_all_embeddingd(words):\r\n","    all_files = []\r\n","    embeddings = {}\r\n","    for w in words:\r\n","        embeddings[w] = []\r\n","\r\n","    for i in words:\r\n","        all_files += from_files[i]\r\n","\r\n","    for name in set(all_files):\r\n","        with open(\"./embeddings/sent1/bin/{}\".format(name), 'rb') as f:\r\n","            data = pc.load(f)\r\n","        for w in words:\r\n","            if w in data:\r\n","                embeddings[w] += data[w]\r\n","    return embeddings\r\n","\r\n","def sum_embeddings(embeddings):\r\n","    res = {}\r\n","    res_emb = {}\r\n","    for i in embeddings:\r\n","        res[i] = torch.empty(len(embeddings[i]), embeddings[i][0].size(0))\r\n","        for j in range(len(embeddings[i])):\r\n","            res[i][j] = embeddings[i][j]\r\n","        res_emb[i] = res[i].mean(dim = 0)\r\n","    return res_emb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IgRmaNBcBu_J","executionInfo":{"status":"error","timestamp":1612906007424,"user_tz":-180,"elapsed":39029070,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"d8a5f98d-ebcf-402a-d035-ebd0356b5f07"},"source":["%%time\r\n","st_time = time.time()\r\n","all_emb = {}\r\n","batch = 100\r\n","for b in qwe:\r\n","    t_time = time.time()\r\n","    words = all_words[b*batch:(b+1)*batch]\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)\r\n","    \r\n","    with open(\"./embeddings/sent1/ready/{}_{:05d}.pic\".format(batch, b), 'wb') as f:\r\n","        pc.dump(res_emb, f)\r\n","\r\n","    print(\"done: {} --- time: {}\\t--- total: {}\".format(b,time.time() - t_time, time.time() - st_time))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["done: 224 --- time: 817.1793594360352\t--- total: 817.1793820858002\n","done: 223 --- time: 831.973221540451\t--- total: 1649.1622314453125\n","done: 222 --- time: 834.4274055957794\t--- total: 2483.5973081588745\n","done: 221 --- time: 845.2402245998383\t--- total: 3328.8412482738495\n","done: 220 --- time: 837.8794119358063\t--- total: 4166.724846124649\n","done: 219 --- time: 833.7412967681885\t--- total: 5000.470985412598\n","done: 218 --- time: 849.2266790866852\t--- total: 5849.704727172852\n","done: 217 --- time: 855.5437593460083\t--- total: 6705.253237247467\n","done: 216 --- time: 854.2998275756836\t--- total: 7559.560549736023\n","done: 215 --- time: 847.3836131095886\t--- total: 8406.951870441437\n","done: 214 --- time: 838.4047076702118\t--- total: 9245.361767053604\n","done: 213 --- time: 849.2518775463104\t--- total: 10094.620011806488\n","done: 212 --- time: 835.9989988803864\t--- total: 10930.626053094864\n","done: 211 --- time: 837.0105073451996\t--- total: 11767.64439868927\n","done: 210 --- time: 840.8120815753937\t--- total: 12608.466158866882\n","done: 209 --- time: 833.7382318973541\t--- total: 13442.212361097336\n","done: 208 --- time: 850.8289406299591\t--- total: 14293.049159765244\n","done: 207 --- time: 841.872808933258\t--- total: 15134.929371118546\n","done: 206 --- time: 850.9613566398621\t--- total: 15985.898681163788\n","done: 205 --- time: 865.0485680103302\t--- total: 16850.95211839676\n","done: 204 --- time: 855.1808724403381\t--- total: 17706.140347480774\n","done: 203 --- time: 859.7234899997711\t--- total: 18565.873577594757\n","done: 202 --- time: 861.3553545475006\t--- total: 19427.239465236664\n","done: 201 --- time: 843.2746138572693\t--- total: 20270.518253803253\n","done: 200 --- time: 854.0848715305328\t--- total: 21124.613930940628\n","done: 199 --- time: 845.7626779079437\t--- total: 21970.386279821396\n","done: 198 --- time: 858.3553433418274\t--- total: 22828.75128865242\n","done: 197 --- time: 846.9985067844391\t--- total: 23675.75865507126\n","done: 196 --- time: 845.4539513587952\t--- total: 24521.217280626297\n","done: 195 --- time: 862.7085633277893\t--- total: 25383.935299158096\n","done: 194 --- time: 842.8143961429596\t--- total: 26226.759099006653\n","done: 193 --- time: 855.8569853305817\t--- total: 27082.623738765717\n","done: 192 --- time: 849.1857557296753\t--- total: 27931.816415071487\n","done: 191 --- time: 845.2051930427551\t--- total: 28777.026913881302\n","done: 190 --- time: 855.8577620983124\t--- total: 29632.890502929688\n","done: 189 --- time: 858.2802872657776\t--- total: 30491.178819656372\n","done: 188 --- time: 869.8956570625305\t--- total: 31361.08193206787\n","done: 187 --- time: 850.3485889434814\t--- total: 32211.435219287872\n","done: 186 --- time: 855.0696876049042\t--- total: 33066.5122358799\n","done: 185 --- time: 869.8371798992157\t--- total: 33936.35710787773\n","done: 184 --- time: 855.0091185569763\t--- total: 34791.37401556969\n","done: 183 --- time: 867.7334504127502\t--- total: 35659.11242198944\n","done: 182 --- time: 850.7268304824829\t--- total: 36509.84470868111\n","done: 181 --- time: 850.6695663928986\t--- total: 37360.51873731613\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b1ded0d0e7c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'st_time = time.time()\\nall_emb = {}\\nbatch = 100\\nfor b in qwe:\\n    t_time = time.time()\\n    words = all_words[b*batch:(b+1)*batch]\\n    emb = get_all_embeddingd(words)\\n    res_emb = sum_embeddings(emb)\\n    \\n    with open(\"./embeddings/sent1/ready/{}_{:05d}.pic\".format(batch, b), \\'wb\\') as f:\\n        pc.dump(res_emb, f)\\n\\n    print(\"done: {} --- time: {}\\\\t--- total: {}\".format(b,time.time() - t_time, time.time() - st_time))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-14-7688f3abe906>\u001b[0m in \u001b[0;36mget_all_embeddingd\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./embeddings/sent1/bin/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"qgbWPFkV8Ctt"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3F7kbbmrH4sT"},"source":["## get embeddings"]},{"cell_type":"code","metadata":{"id":"z5KEphwROsy_"},"source":["%%time\r\n","# words = ['мать', 'сын', 'отец', 'дочь', 'парень', 'девушка', 'машина', 'автомобиль', 'самолет', 'ребенок', 'королева', 'король', 'лед', 'тепло', 'вода']\r\n","words = ['россия','москв','франци','париж','украин','киев']\r\n","\r\n","if check_existing_emb(words):\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xwUIz_vUNsm"},"source":["t1 = res_emb['франци'] - res_emb['париж'] + res_emb['киев']\r\n","t2 = res_emb['украин']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuB9zah9UN9F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2ipTLE-UOCG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMcKl9gPRkjY"},"source":["mother = res_emb['мать']\r\n","father = res_emb['отец']\r\n","sun = res_emb['сын']\r\n","dot = res_emb['дочь']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTA0HYGB-h_Y"},"source":["t1 = res_emb['девушка'] - res_emb['парень'] + res_emb['сын']\r\n","t2 = res_emb['дочь']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uElQlw28IbbD"},"source":["t1 = res_emb['король'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['королева']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Y2OMQFcIvtb"},"source":["t1 = res_emb['король'] - res_emb['королева']\r\n","t2 = res_emb['девушка'] - res_emb['парень']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kH7PgjWtB9eY"},"source":["t1 = res_emb['отец'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['мать']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SSwhPJm-nqL"},"source":["t1 = res_emb['девушка']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PrvWbb7B3HC"},"source":["t1 = res_emb['машина']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnurMtwbUy78"},"source":["t1 = mother - father + sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzR_wCkPUzAn"},"source":["t1 = mother\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ot_kYH5cUzDh"},"source":["t1 = father\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DJuzMl3UzGy"},"source":["t1 = sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trqPoVJbUzJo"},"source":["t1 = mother\r\n","t2 = father\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lZ3FMjEcUP4"},"source":["t1 = mother\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_mmvAJ1cUT7"},"source":["t1 = father\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTzf59SKXjex"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJ1Jbokqml8X"},"source":["# short "]},{"cell_type":"markdown","metadata":{"id":"mSnTVO95wEqL"},"source":["## imports"]},{"cell_type":"code","metadata":{"id":"iCBwhjwymmux"},"source":["import torch\r\n","import nltk\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiaAhMe7nU6N"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0SKv0m8warN"},"source":["from razdel import sentenize\r\n","# list(sentenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n98R8rEWxSOJ"},"source":["\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","# sentence = \"\"\"At eight o'clock on Thursday morning. Arthur didn't feel very good.\"\"\"\r\n","tokens = nltk.word_tokenize(sentences[1])\r\n","tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lj1Lu9WPyRk1"},"source":["sentences = nltk.sent_tokenize(text)\r\n","sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqaOLbQ3xb1V"},"source":["tagged = nltk.pos_tag(tokens)\r\n","tagged[0:6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADtaVlDjwH9p"},"source":["## work"]},{"cell_type":"code","metadata":{"id":"A_5phDHumt1i"},"source":["text = 'Замок стоял неприступный. Замок легко открылся.'\r\n","text2 = \"At eight o'clock on Thursday morning. Arthur didn't feel very good.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2BCGpfIeO1G"},"source":["# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n","\r\n","# marked_text2 = \"[CLS] \" + \"Работает по следующему принципу\" + \" [SEP]\"\r\n","# tokenized_text2 = tokenizer.tokenize(marked_text2)\r\n","# tokenized_text2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1TawqaTmt5A"},"source":["marked_text2 = text.lower()\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","# tokenized_text = nltk.word_tokenize(marked_text)\r\n","\r\n","# tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\r\n","\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKlj14WFaL6V"},"source":["tokenized_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wPr8IKKmt_3"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f0yAQrsnigF"},"source":["with torch.no_grad():\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQOs50kOnisy"},"source":["# token_i = 5\r\n","# layer_i = 5\r\n","# vec = hidden_states[layer_i][batch_i][token_i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfKPm4a2gMRc"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFmJdLgJnui8"},"source":["# Remove dimension 1, the \"batches\".\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1).permute(1,0,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh65R4Mun7dX"},"source":["token_vecs_sum = []\r\n","for token in token_embeddings:\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5b6OwZDqqhR"},"source":["len(token_vecs_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za7aF1iQrYJE"},"source":["for i, token_str in enumerate(tokenized_text):\r\n","  print (i, token_str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThBR9wTfoA4k"},"source":["token_vecs = hidden_states[-2][0]\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEjdLKPEbm2J"},"source":["token_vecs.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfHWSz36oA78"},"source":["diff_bank = 1 - cosine(token_vecs_sum[1], token_vecs_sum[7])\r\n","diff_bank1 = 1 - cosine(token_vecs[1], token_vecs[7])\r\n","# same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank1)\r\n","# print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXtCWy7idfs5"},"source":["# Baseline\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"1vz_xfmsdqX_"},"source":["import torch\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CaU-dkwpdyar"},"source":["text = \"Here is the sentence I want embeddings for.\"\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Tokenize our sentence with the BERT tokenizer.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Print out the tokens.\r\n","print (tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDdQ7uDRd5R4"},"source":["# Define a new example sentence with multiple meanings of the word \"bank\"\r\n","text = \"After stealing money from the bank vault, the bank robber was seen \" \\\r\n","       \"fishing on the Mississippi river bank.\"\r\n","\r\n","# Add the special tokens.\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Split the sentence into tokens.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Map the token strings to their vocabulary indeces.\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","\r\n","# Display the words with their indeces.\r\n","for tup in zip(tokenized_text, indexed_tokens):\r\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sQM8BeAeP0Z"},"source":["# Mark each of the 22 tokens as belonging to sentence \"1\".\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcZ2TlS6eCHo"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fWcVbNNeMHa"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('bert-base-uncased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp_jYk_5eUB3"},"source":["# Run the text through BERT, and collect all of the hidden states produced\r\n","# from all 12 layers. \r\n","with torch.no_grad():\r\n","\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","\r\n","    # Evaluating the model will return a different number of objects based on \r\n","    # how it's  configured in the `from_pretrained` call earlier. In this case, \r\n","    # becase we set `output_hidden_states = True`, the third item will be the \r\n","    # hidden states from all layers. See the documentation for more details:\r\n","    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOImPmmufFeO"},"source":["# outputs.shape\r\n","hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlk64upJfIP4"},"source":["print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\r\n","layer_i = 0\r\n","\r\n","print (\"Number of batches:\", len(hidden_states[layer_i]))\r\n","batch_i = 0\r\n","\r\n","print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\r\n","token_i = 0\r\n","\r\n","print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYDKqBYNhOGZ"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD82_lhOky7g"},"source":["# Remove dimension 1, the \"batches\".\r\n","\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\r\n","\r\n","token_embeddings.size()\r\n","\r\n","# Swap dimensions 0 and 1.\r\n","token_embeddings = token_embeddings.permute(1,0,2)\r\n","\r\n","token_embeddings.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UT-Pbq7hhzV"},"source":["# Stores the token vectors, with shape [22 x 3,072]\r\n","token_vecs_cat = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","    \r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Concatenate the vectors (that is, append them together) from the last \r\n","    # four layers.\r\n","    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\r\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\r\n","    \r\n","    # Use `cat_vec` to represent `token`.\r\n","    token_vecs_cat.append(cat_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2mSMHsghUkx"},"source":["# Stores the token vectors, with shape [22 x 768]\r\n","token_vecs_sum = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","\r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Sum the vectors from the last four layers.\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    \r\n","    # Use `sum_vec` to represent `token`.\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2ZHmxSFlNDt"},"source":["# `hidden_states` has shape [13 x 1 x 22 x 768]\r\n","\r\n","# `token_vecs` is a tensor with shape [22 x 768]\r\n","token_vecs = hidden_states[-2][0]\r\n","\r\n","# Calculate the average of all 22 token vectors.\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRdzQAmilRws"},"source":["from scipy.spatial.distance import cosine\r\n","\r\n","# Calculate the cosine similarity between the word bank \r\n","# in \"bank robber\" vs \"river bank\" (different meanings).\r\n","diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\r\n","\r\n","# Calculate the cosine similarity between the word bank\r\n","# in \"bank robber\" vs \"bank vault\" (same meaning).\r\n","same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\r\n","print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sLrZsOclZSI"},"source":[""],"execution_count":null,"outputs":[]}]}