{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hse_BERT_2_4.ipynb","provenance":[{"file_id":"1BPl1egIELuCLwWGpQZWpSz3aU1QRd041","timestamp":1613932894782},{"file_id":"1nzCCrBX7JEXLTCvgpBecsmtptKx9Jesf","timestamp":1613932771884},{"file_id":"1AtbtIhxWMBlu2gfk3SIMM1nMn5Ce4A8q","timestamp":1613930303684}],"authorship_tag":"ABX9TyPS78RFiNLn//aR8qO0gWnj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wFOv_GdndRBf"},"source":["# Подготовка среды"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNIRbsjvXTT1","executionInfo":{"status":"ok","timestamp":1613971877532,"user_tz":-180,"elapsed":52267,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"b970a816-0f9e-44a8-e130-f0d77f01642e"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e3Jn3fsZ3oVD","executionInfo":{"status":"ok","timestamp":1613971877534,"user_tz":-180,"elapsed":3637,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import pickle as pc\r\n","import numpy as np\r\n","import numpy"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrMRUKMUZC-W","executionInfo":{"status":"ok","timestamp":1613971877784,"user_tz":-180,"elapsed":3660,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import os\r\n","os.chdir('/content/drive/Shared drives/hse_BERT/hse_Af_Tr_BERT')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXjr0XyrdpLz"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrzflXz2wdQG","executionInfo":{"status":"ok","timestamp":1613971887939,"user_tz":-180,"elapsed":13390,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"38524f4f-ccae-430a-f3cc-268c8a008d1f"},"source":["!pip install razdel"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting razdel\n","  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n","Installing collected packages: razdel\n","Successfully installed razdel-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zSA0MOOMTPgk","executionInfo":{"status":"ok","timestamp":1613971891798,"user_tz":-180,"elapsed":17063,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["import torch\r\n","import nltk\r\n","import re\r\n","import json\r\n","import time\r\n","import numpy as np\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ge0GhZA0zzE"},"source":["# read embeddings"]},{"cell_type":"code","metadata":{"id":"OJxSs8oa00QK","executionInfo":{"status":"ok","timestamp":1613971896963,"user_tz":-180,"elapsed":21773,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["files = os.listdir(\"./embeddings/sent2/bin\")\r\n","cut_files = []\r\n","for i in files:\r\n","    if i.split('.')[-1] == 'pic':\r\n","        cut_files.append(i)\r\n","# cut_files"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_kbzYi51sTW","executionInfo":{"status":"ok","timestamp":1613971896968,"user_tz":-180,"elapsed":21541,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["# i = \"1000_00024.pic\"\r\n","# for i in cut_files:\r\n","#     with open(\"./embeddings/sent1/bin/{}\".format(i), 'rb') as f:\r\n","#         data = pc.load(f)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHu7WxpNH0Fe"},"source":["##get count"]},{"cell_type":"code","metadata":{"id":"pV-8Gucl1sZd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613973539394,"user_tz":-180,"elapsed":1663511,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"b838dc06-5844-470f-8b27-fe39751f6306"},"source":["%%time\r\n","from_files = {}\r\n","all_count = {}\r\n","\r\n","for name in cut_files:\r\n","    with open(\"./embeddings/sent2/bin/{}\".format(name), 'rb') as f:\r\n","        data = pc.load(f)\r\n","    for i in data.keys():\r\n","        if i not in all_count:\r\n","            all_count[i] = len(data[i])\r\n","            from_files[i] = [name]\r\n","        else:\r\n","            all_count[i] += len(data[i])\r\n","            from_files[i].append(name)\r\n","    # print(name)\r\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CPU times: user 13min 30s, sys: 31.5 s, total: 14min 1s\n","Wall time: 27min 22s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QS6qXPsK1sc_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613973539397,"user_tz":-180,"elapsed":1662998,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"3e950095-4062-4fbd-8592-412acad74fae"},"source":["out = list(zip(all_count.keys(), list(all_count.values())))\r\n","out.sort(key = lambda i: i[1], reverse=True)\r\n","out[:20]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('что', 123060),\n"," ('как', 99719),\n"," ('для', 72989),\n"," ('росс', 58015),\n"," ('это', 50965),\n"," ('the', 40895),\n"," ('или', 40470),\n"," ('его', 39013),\n"," ('власти', 37854),\n"," ('политической', 33833),\n"," ('так', 33127),\n"," ('при', 32306),\n"," ('более', 29142),\n"," ('между', 28387),\n"," ('также', 27566),\n"," ('все', 27544),\n"," ('только', 26695),\n"," ('государства', 26039),\n"," ('политических', 25456),\n"," ('является', 24792)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"_tMbqY5CFNH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613973539398,"user_tz":-180,"elapsed":1662734,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"55ac9a48-852b-41fc-d311-fe5cc367f508"},"source":["all_count[\"москв\"]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6459"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"wvOtbW-6a4np","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613973539400,"user_tz":-180,"elapsed":1661990,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"ceb1aecc-0a8a-4bdb-a994-0c278962dcc7"},"source":["keys = list(all_count.keys())\r\n","k1 = []\r\n","k2 = []\r\n","k3 = []\r\n","kb = []\r\n","l = 0\r\n","for i in keys:\r\n","    l = len(i)\r\n","    if l == 1:\r\n","        k1.append(i)\r\n","    elif l == 2:\r\n","        k2.append(i)\r\n","    elif l == 3:\r\n","        k3.append(i)\r\n","    else:\r\n","        kb.append(i)\r\n","len(k1),len(k2),len(k3),len(kb)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0, 2068, 52527)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"TIHeXRv2cbM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613973539401,"user_tz":-180,"elapsed":1661432,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"c6485aa5-f19e-4c15-edc9-a38566f8d2a9"},"source":["all_words = []\r\n","for i in kb:\r\n","    if all_count[i] > 10:\r\n","        all_words.append(i)\r\n","len(all_words)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37553"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"zzUkB5fhHbDt","executionInfo":{"status":"ok","timestamp":1613973539402,"user_tz":-180,"elapsed":1660954,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}}},"source":["def check_existing_emb(words):\r\n","    d = dict(out)\r\n","    fl = True\r\n","    for i in words:\r\n","        if i not in d:\r\n","            print(\"{} not in dictionary\".format(i))\r\n","            fl = False\r\n","    return fl\r\n","\r\n","def get_all_embeddingd(words):\r\n","    all_files = []\r\n","    embeddings = {}\r\n","    for w in words:\r\n","        embeddings[w] = []\r\n","\r\n","    for i in words:\r\n","        all_files += from_files[i]\r\n","\r\n","    for name in set(all_files):\r\n","        with open(\"./embeddings/sent2/bin/{}\".format(name), 'rb') as f:\r\n","            data = pc.load(f)\r\n","        for w in words:\r\n","            if w in data:\r\n","                embeddings[w] += data[w]\r\n","    return embeddings\r\n","\r\n","def sum_embeddings(embeddings):\r\n","    res = {}\r\n","    res_emb = {}\r\n","    for i in embeddings:\r\n","        res[i] = torch.empty(len(embeddings[i]), embeddings[i][0].size(0))\r\n","        for j in range(len(embeddings[i])):\r\n","            res[i][j] = embeddings[i][j]\r\n","        res_emb[i] = res[i].mean(dim = 0)\r\n","    return res_emb"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4QVUqOVv_8l","executionInfo":{"status":"ok","timestamp":1613973539403,"user_tz":-180,"elapsed":1660417,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"c59f8009-b1ac-476a-92f4-758d4213efc7"},"source":["len(all_words)//100"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["375"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"IgRmaNBcBu_J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614004863616,"user_tz":-180,"elapsed":1444553,"user":{"displayName":"Андрей Солодянкин","photoUrl":"","userId":"09767960549124606880"}},"outputId":"27e3c0d4-9641-4f39-f5bd-2df6ea2f3741"},"source":["%%time\r\n","st_time = time.time()\r\n","all_emb = {}\r\n","batch = 1000\r\n","for b in range(14, 15):\r\n","    t_time = time.time()\r\n","    words = all_words[b*batch:(b+1)*batch ]\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)\r\n","    \r\n","    with open(\"./embeddings/sent2/ready/{}_{:05d}.pic\".format(batch, b), 'wb') as f:\r\n","        pc.dump(res_emb, f)\r\n","\r\n","    print(\"done: {} --- time: {}\\t--- total: {}\".format(b,time.time() - t_time, time.time() - st_time))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["done: 14 --- time: 1443.7675478458405\t--- total: 1443.7675535678864\n","CPU times: user 15min 19s, sys: 32.2 s, total: 15min 51s\n","Wall time: 24min 3s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qgbWPFkV8Ctt"},"source":[" words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3F7kbbmrH4sT"},"source":["## get embeddings"]},{"cell_type":"code","metadata":{"id":"z5KEphwROsy_"},"source":["%%time\r\n","# words = ['мать', 'сын', 'отец', 'дочь', 'парень', 'девушка', 'машина', 'автомобиль', 'самолет', 'ребенок', 'королева', 'король', 'лед', 'тепло', 'вода']\r\n","words = ['россия','москв','франци','париж','украин','киев']\r\n","\r\n","if check_existing_emb(words):\r\n","    emb = get_all_embeddingd(words)\r\n","    res_emb = sum_embeddings(emb)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xwUIz_vUNsm"},"source":["t1 = res_emb['франци'] - res_emb['париж'] + res_emb['киев']\r\n","t2 = res_emb['украин']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuB9zah9UN9F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2ipTLE-UOCG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMcKl9gPRkjY"},"source":["mother = res_emb['мать']\r\n","father = res_emb['отец']\r\n","sun = res_emb['сын']\r\n","dot = res_emb['дочь']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTA0HYGB-h_Y"},"source":["t1 = res_emb['девушка'] - res_emb['парень'] + res_emb['сын']\r\n","t2 = res_emb['дочь']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uElQlw28IbbD"},"source":["t1 = res_emb['король'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['королева']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Y2OMQFcIvtb"},"source":["t1 = res_emb['король'] - res_emb['королева']\r\n","t2 = res_emb['девушка'] - res_emb['парень']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kH7PgjWtB9eY"},"source":["t1 = res_emb['отец'] - res_emb['парень'] + res_emb['девушка']\r\n","t2 = res_emb['мать']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SSwhPJm-nqL"},"source":["t1 = res_emb['девушка']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PrvWbb7B3HC"},"source":["t1 = res_emb['машина']\r\n","t2 = res_emb['автомобиль']\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnurMtwbUy78"},"source":["t1 = mother - father + sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzR_wCkPUzAn"},"source":["t1 = mother\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ot_kYH5cUzDh"},"source":["t1 = father\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DJuzMl3UzGy"},"source":["t1 = sun\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trqPoVJbUzJo"},"source":["t1 = mother\r\n","t2 = father\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lZ3FMjEcUP4"},"source":["t1 = mother\r\n","t2 = sun\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_mmvAJ1cUT7"},"source":["t1 = father\r\n","t2 = dot\r\n","diff_bank = 1 - cosine(t1, t2)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTzf59SKXjex"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJ1Jbokqml8X"},"source":["# short "]},{"cell_type":"markdown","metadata":{"id":"mSnTVO95wEqL"},"source":["## imports"]},{"cell_type":"code","metadata":{"id":"iCBwhjwymmux"},"source":["import torch\r\n","import nltk\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","from scipy.spatial.distance import cosine\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiaAhMe7nU6N"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0SKv0m8warN"},"source":["from razdel import sentenize\r\n","# list(sentenize(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n98R8rEWxSOJ"},"source":["\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","# sentence = \"\"\"At eight o'clock on Thursday morning. Arthur didn't feel very good.\"\"\"\r\n","tokens = nltk.word_tokenize(sentences[1])\r\n","tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lj1Lu9WPyRk1"},"source":["sentences = nltk.sent_tokenize(text)\r\n","sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqaOLbQ3xb1V"},"source":["tagged = nltk.pos_tag(tokens)\r\n","tagged[0:6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADtaVlDjwH9p"},"source":["## work"]},{"cell_type":"code","metadata":{"id":"A_5phDHumt1i"},"source":["text = 'Замок стоял неприступный. Замок легко открылся.'\r\n","text2 = \"At eight o'clock on Thursday morning. Arthur didn't feel very good.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2BCGpfIeO1G"},"source":["# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n","\r\n","# marked_text2 = \"[CLS] \" + \"Работает по следующему принципу\" + \" [SEP]\"\r\n","# tokenized_text2 = tokenizer.tokenize(marked_text2)\r\n","# tokenized_text2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1TawqaTmt5A"},"source":["marked_text2 = text.lower()\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","# tokenized_text = nltk.word_tokenize(marked_text)\r\n","\r\n","# tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\r\n","\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKlj14WFaL6V"},"source":["tokenized_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wPr8IKKmt_3"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f0yAQrsnigF"},"source":["with torch.no_grad():\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQOs50kOnisy"},"source":["# token_i = 5\r\n","# layer_i = 5\r\n","# vec = hidden_states[layer_i][batch_i][token_i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfKPm4a2gMRc"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFmJdLgJnui8"},"source":["# Remove dimension 1, the \"batches\".\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1).permute(1,0,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh65R4Mun7dX"},"source":["token_vecs_sum = []\r\n","for token in token_embeddings:\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5b6OwZDqqhR"},"source":["len(token_vecs_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za7aF1iQrYJE"},"source":["for i, token_str in enumerate(tokenized_text):\r\n","  print (i, token_str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThBR9wTfoA4k"},"source":["token_vecs = hidden_states[-2][0]\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEjdLKPEbm2J"},"source":["token_vecs.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfHWSz36oA78"},"source":["diff_bank = 1 - cosine(token_vecs_sum[1], token_vecs_sum[7])\r\n","diff_bank1 = 1 - cosine(token_vecs[1], token_vecs[7])\r\n","# same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank)\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % diff_bank1)\r\n","# print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXtCWy7idfs5"},"source":["# Baseline\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"1vz_xfmsdqX_"},"source":["import torch\r\n","from transformers import BertTokenizer, BertModel\r\n","\r\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\r\n","import logging\r\n","#logging.basicConfig(level=logging.INFO)\r\n","\r\n","import matplotlib.pyplot as plt\r\n","% matplotlib inline\r\n","\r\n","# Load pre-trained model tokenizer (vocabulary)\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CaU-dkwpdyar"},"source":["text = \"Here is the sentence I want embeddings for.\"\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Tokenize our sentence with the BERT tokenizer.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Print out the tokens.\r\n","print (tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDdQ7uDRd5R4"},"source":["# Define a new example sentence with multiple meanings of the word \"bank\"\r\n","text = \"After stealing money from the bank vault, the bank robber was seen \" \\\r\n","       \"fishing on the Mississippi river bank.\"\r\n","\r\n","# Add the special tokens.\r\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\r\n","\r\n","# Split the sentence into tokens.\r\n","tokenized_text = tokenizer.tokenize(marked_text)\r\n","\r\n","# Map the token strings to their vocabulary indeces.\r\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n","\r\n","# Display the words with their indeces.\r\n","for tup in zip(tokenized_text, indexed_tokens):\r\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sQM8BeAeP0Z"},"source":["# Mark each of the 22 tokens as belonging to sentence \"1\".\r\n","segments_ids = [1] * len(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcZ2TlS6eCHo"},"source":["# Convert inputs to PyTorch tensors\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fWcVbNNeMHa"},"source":["# Load pre-trained model (weights)\r\n","model = BertModel.from_pretrained('bert-base-uncased',\r\n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\r\n","                                  )\r\n","\r\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\r\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp_jYk_5eUB3"},"source":["# Run the text through BERT, and collect all of the hidden states produced\r\n","# from all 12 layers. \r\n","with torch.no_grad():\r\n","\r\n","    outputs = model(tokens_tensor, segments_tensors)\r\n","\r\n","    # Evaluating the model will return a different number of objects based on \r\n","    # how it's  configured in the `from_pretrained` call earlier. In this case, \r\n","    # becase we set `output_hidden_states = True`, the third item will be the \r\n","    # hidden states from all layers. See the documentation for more details:\r\n","    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\r\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOImPmmufFeO"},"source":["# outputs.shape\r\n","hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlk64upJfIP4"},"source":["print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\r\n","layer_i = 0\r\n","\r\n","print (\"Number of batches:\", len(hidden_states[layer_i]))\r\n","batch_i = 0\r\n","\r\n","print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\r\n","token_i = 0\r\n","\r\n","print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYDKqBYNhOGZ"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\r\n","token_i = 5\r\n","layer_i = 5\r\n","vec = hidden_states[layer_i][batch_i][token_i]\r\n","\r\n","# Plot the values as a histogram to show their distribution.\r\n","plt.figure(figsize=(10,10))\r\n","plt.hist(vec, bins=200)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD82_lhOky7g"},"source":["# Remove dimension 1, the \"batches\".\r\n","\r\n","token_embeddings = torch.stack(hidden_states, dim=0)\r\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\r\n","\r\n","token_embeddings.size()\r\n","\r\n","# Swap dimensions 0 and 1.\r\n","token_embeddings = token_embeddings.permute(1,0,2)\r\n","\r\n","token_embeddings.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UT-Pbq7hhzV"},"source":["# Stores the token vectors, with shape [22 x 3,072]\r\n","token_vecs_cat = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","    \r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Concatenate the vectors (that is, append them together) from the last \r\n","    # four layers.\r\n","    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\r\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\r\n","    \r\n","    # Use `cat_vec` to represent `token`.\r\n","    token_vecs_cat.append(cat_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2mSMHsghUkx"},"source":["# Stores the token vectors, with shape [22 x 768]\r\n","token_vecs_sum = []\r\n","\r\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\r\n","\r\n","# For each token in the sentence...\r\n","for token in token_embeddings:\r\n","\r\n","    # `token` is a [12 x 768] tensor\r\n","\r\n","    # Sum the vectors from the last four layers.\r\n","    sum_vec = torch.sum(token[-4:], dim=0)\r\n","    \r\n","    # Use `sum_vec` to represent `token`.\r\n","    token_vecs_sum.append(sum_vec)\r\n","\r\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2ZHmxSFlNDt"},"source":["# `hidden_states` has shape [13 x 1 x 22 x 768]\r\n","\r\n","# `token_vecs` is a tensor with shape [22 x 768]\r\n","token_vecs = hidden_states[-2][0]\r\n","\r\n","# Calculate the average of all 22 token vectors.\r\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRdzQAmilRws"},"source":["from scipy.spatial.distance import cosine\r\n","\r\n","# Calculate the cosine similarity between the word bank \r\n","# in \"bank robber\" vs \"river bank\" (different meanings).\r\n","diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\r\n","\r\n","# Calculate the cosine similarity between the word bank\r\n","# in \"bank robber\" vs \"bank vault\" (same meaning).\r\n","same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\r\n","\r\n","print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\r\n","print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sLrZsOclZSI"},"source":[""],"execution_count":null,"outputs":[]}]}